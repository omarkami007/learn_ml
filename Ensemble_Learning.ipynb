{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsiMQs8+1+J56otNAjZD4n"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Learning**\n",
        "\n",
        "The idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models. In this section, we will explore:\n",
        "1. **Bagging**\n",
        "  * 1.1 **Random Forests**\n",
        "2. **Boosting**\n",
        "3. **Stacking**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Bagging (Bootstrap Aggregation)**\n",
        "\n",
        "### **How It Works**\n",
        "- Create multiple datasets by sampling the original dataset **with replacement** (bootstrap sampling).\n",
        "- Train separate models on each dataset.\n",
        "- Combine the predictions (e.g., **majority vote** for classification or **averaging** for regression).\n",
        "\n",
        "**Example**: Random Forests.  \n",
        "**Key Idea**: Reduces **variance** by averaging predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1 Random Forests**\n",
        "\n",
        "Random forests are a special case of bagging. While building the bootstrapped $B$ trees, $m$ features ($m < p$, where $p$ is the total number of features) are randomly selected at each split. This reduces the correlation between the trees, which helps to **further reduce variance**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Algorithm: Random Forest for Regression or Classification**\n",
        "\n",
        "1. For $b = 1$ to $B$:\n",
        "    - (a) Draw a **bootstrap sample** $Z^*$ of size $N$ from the training data.\n",
        "    - (b) Grow a random-forest tree $T_b$ on the bootstrapped data by recursively repeating the following steps for each terminal node of the tree, until the minimum node size $n_{\\text{min}}$ is reached:\n",
        "        1. Randomly select $m$ variables from the $p$ total variables.\n",
        "        2. Pick the **best variable** and corresponding **split-point** among the $m$.\n",
        "        3. Split the node into two daughter nodes.\n",
        "\n",
        "2. Output the ensemble of trees $\\{T_b\\}_{b=1}^B$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Prediction**\n",
        "\n",
        "- **Regression**:  \n",
        "  The prediction for a new point $x$ is the **average** of the predictions from all trees:\n",
        "  $\n",
        "  \\hat{f}^B_{\\text{rf}}(x) = \\frac{1}{B} \\sum_{b=1}^B T_b(x)\n",
        "  $\n",
        "\n",
        "- **Classification**:  \n",
        "  The prediction for a new point $x$ is the **majority vote** across all trees:\n",
        " $\n",
        "  \\hat{C}^B_{\\text{rf}}(x) = \\text{majority vote} \\{\\hat{C}_b(x)\\}_{b=1}^B\n",
        " $\n",
        "\n",
        "\n",
        "## **2. Boosting**\n",
        "\n",
        "Boosting is a type of ensemble learning where **weak learners** are trained **sequentially**, with each learner focusing on the errors of the previous ones. The goal is to combine these weak learners to create a strong, accurate model.\n",
        "\n",
        "---\n",
        "\n",
        "### **What is a Weak Learner?**\n",
        "A weak learner is a model that performs slightly better than random guessing. For example, a decision tree with only one split (also known as a decision stump) is a weak learner.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Boosting Works**\n",
        "1. Train the first model (weak learner).\n",
        "2. Identify the **errors** made by the model.\n",
        "3. Assign **higher weights** to the misclassified examples, so the next model focuses more on these difficult cases.\n",
        "4. Repeat for a predefined number of iterations or until performance stops improving.\n",
        "5. Combine the predictions of all weak learners to make a final prediction.\n",
        "\n",
        "**Key Point**: Unlike bagging, boosting does not use bootstrap sampling. Instead, it reuses the entire dataset in each iteration with **adjusted weights** to emphasize errors.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1 AdaBoost: Algorithm**\n",
        "\n",
        "#### **Steps**:\n",
        "1. **Initialize** the observation weights:\n",
        "   $\n",
        "   w_i = \\frac{1}{N}, \\, i = 1, 2, \\ldots, N.\n",
        "   $\n",
        "\n",
        "2. **For** $m = 1$ to $M$:\n",
        "    - (a) Fit a weak classifier $G_m(x)$ to the training data using weights $w_i$.\n",
        "    - (b) Compute the weighted classification error:\n",
        "      $\n",
        "      \\text{err}_m = \\frac{\\sum_{i=1}^N w_i \\, I(y_i \\neq G_m(x_i))}{\\sum_{i=1}^N w_i}.\n",
        "      $\n",
        "    - (c) Compute the classifier weight:\n",
        "      $\n",
        "      \\alpha_m = \\log\\left(\\frac{1 - \\text{err}_m}{\\text{err}_m}\\right).\n",
        "      $\n",
        "    - (d) Update the observation weights:\n",
        "      $\n",
        "      w_i \\leftarrow w_i \\cdot \\exp\\left(\\alpha_m \\cdot I(y_i \\neq G_m(x_i))\\right), \\, i = 1, 2, \\ldots, N.\n",
        "      $\n",
        "\n",
        "3. **Output** the final strong classifier:\n",
        "   $\n",
        "   G(x) = \\text{sign}\\left(\\sum_{m=1}^M \\alpha_m G_m(x)\\right).\n",
        "   $\n",
        "\n",
        "\n",
        "### **Strengths of Boosting**\n",
        "- **High accuracy**: Can produce highly accurate models by combining weak learners.\n",
        "- **Versatility**: Works well for both classification and regression tasks.\n",
        "- **Robust to overfitting**: With proper regularization, boosting methods are less prone to overfitting than individual learners.\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations**\n",
        "- **Sensitive to noisy data**: Boosting may overfit on mislabeled examples.\n",
        "- **Computational cost**: Sequential training of weak learners can be slower than parallel methods like bagging."
      ],
      "metadata": {
        "id": "FBN9CGTvmVpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC_M3x68mUAM"
      },
      "outputs": [],
      "source": [
        "# Xgboost to be added later\n",
        "#code to be added later"
      ]
    }
  ]
}