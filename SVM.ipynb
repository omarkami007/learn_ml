{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy9xqfnvtJjzqAa7kkUqh/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Support Vector Machines (SVM)**\n",
        "\n",
        "Support Vector Machines (SVM) are a supervised learning algorithm used for both **classification** and **regression** tasks. SVMs aim to find the **best decision boundary** (hyperplane) that separates data into different classes.\n",
        "\n",
        "---\n",
        "\n",
        "1. **Decision Boundary (Hyperplane)**:\n",
        "   - The hyperplane is the boundary that separates data points of different classes.\n",
        "   - In a 2D space, this is a line; in higher dimensions, it becomes a plane or a hyperplane.\n",
        "\n",
        "2. **Maximizing the Margin**:\n",
        "   - SVM finds the hyperplane that maximizes the **margin** (the distance between the hyperplane and the closest data points of each class).\n",
        "   - The closest data points to the hyperplane are called **support vectors**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Mathematical Formulation**\n",
        "\n",
        "#### **Hyperplane Definition**:\n",
        "A hyperplane is defined as:\n",
        "$\n",
        "w \\cdot x + b = 0\n",
        "$\n",
        "Where:\n",
        "- $w$: Weight vector (perpendicular to the hyperplane).\n",
        "- $x$: Data points (features).\n",
        "- $b$: Bias term (offset from the origin).\n",
        "\n",
        "#### **Optimization Objective**:\n",
        "The goal is to maximize the margin while ensuring that data points are correctly classified:\n",
        "$\n",
        "y_i (w \\cdot x_i + b) \\geq 1, \\quad \\forall i\n",
        "$\n",
        "The margin is inversely proportional to $\\|w\\|$ ( $M = \\frac{1}{\\|w\\|}$), so we minimize:\n",
        "$\n",
        "\\|w\\|\n",
        "$\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Handling Non-Linearly Separable Data**\n",
        "\n",
        "1. **Soft Margin (Slack Variables)**:\n",
        "   - Introduces slack variables $\\xi_i$ to allow some misclassifications.\n",
        "   - The modified optimization problem becomes:\n",
        "     $\n",
        "     \\min \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^N \\xi_i\n",
        "     $\n",
        "     Where $C$ is a regularization parameter controlling the trade-off between maximizing the margin and minimizing misclassifications.\n",
        "\n",
        "2. **Kernel Trick**:\n",
        "   - For data that cannot be separated linearly, SVM uses **kernels** to project the data into a higher-dimensional space where it becomes linearly separable.\n",
        "   - Common kernels include:\n",
        "     - **Linear Kernel**: $K(x_i, x_j) = x_i \\cdot x_j$\n",
        "     - **Polynomial Kernel**: $K(x_i, x_j) = (x_i \\cdot x_j + c)^d$\n",
        "     - **RBF (Gaussian) Kernel**: $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$\n",
        "     - **Sigmoid Kernel**: $K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c)$"
      ],
      "metadata": {
        "id": "QcpdI7cNC0eg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkF6xtasCy4d"
      },
      "outputs": [],
      "source": [
        "#code to be added later"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GWGI3BTmMdr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}